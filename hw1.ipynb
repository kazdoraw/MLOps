{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023f3476",
   "metadata": {},
   "source": [
    "# Домашнее задание 1: ML Pipeline с DVC и MLflow\n",
    "\n",
    "## Цель\n",
    "Создание воспроизводимого ML-pipeline для классификации медицинских консультаций с использованием датасета ChatDoctor\n",
    "\n",
    "## Структура проекта\n",
    "```\n",
    "HW1/\n",
    "├── data/\n",
    "│   ├── raw/          # Исходные данные (DVC)\n",
    "│   └── processed/    # Обработанные данные (DVC)\n",
    "├── src/\n",
    "│   ├── prepare.py    # Подготовка данных\n",
    "│   └── train.py      # Обучение модели\n",
    "├── models/           # Сохраненные модели (DVC)\n",
    "├── dvc.yaml          # Описание пайплайна\n",
    "├── params.yaml       # Гиперпараметры\n",
    "└── requirements.txt  # Зависимости\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc9ef3",
   "metadata": {},
   "source": [
    "## Шаг 1: Установка зависимостей\n",
    "\n",
    "Установим все необходимые библиотеки для работы с DVC, MLflow и ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas numpy scikit-learn mlflow dvc kagglehub joblib matplotlib seaborn pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e907d",
   "metadata": {},
   "source": [
    "## Шаг 2: Загрузка датасета ChatDoctor\n",
    "\n",
    "Загружаем датасет с Kaggle и сохраняем в папку data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "print(\"Загрузка датасета ChatDoctor...\")\n",
    "path = kagglehub.dataset_download(\"punyaslokaprusty/chatdoctor\")\n",
    "print(f\"Датасет загружен в: {path}\")\n",
    "\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(f\"\\nНайдены файлы: {csv_files}\")\n",
    "\n",
    "if csv_files:\n",
    "    source_file = os.path.join(path, csv_files[0])\n",
    "    target_file = \"data/raw/chatdoctor.csv\"\n",
    "    shutil.copy(source_file, target_file)\n",
    "    print(f\"\\nДатасет скопирован в: {target_file}\")\n",
    "else:\n",
    "    print(\"CSV файлы не найдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd85233",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "print(\"Загрузка датасета ChatDoctor...\")\n",
    "path = kagglehub.dataset_download(\"punyaslokaprusty/chatdoctor\")\n",
    "print(f\"Датасет загружен в: {path}\")\n",
    "\n",
    "files = os.listdir(path)\n",
    "print(f\"\\nФайлы в датасете: {files}\")\n",
    "\n",
    "for file in files:\n",
    "    source = os.path.join(path, file)\n",
    "    if os.path.isfile(source):\n",
    "        target = os.path.join(\"data/raw\", file)\n",
    "        shutil.copy(source, target)\n",
    "        print(f\"Скопирован: {file}\")\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(target, nrows=5)\n",
    "            print(f\"\\nСтруктура {file}:\")\n",
    "            print(f\"Колонки: {list(df.columns)}\")\n",
    "            print(f\"Размер: {pd.read_csv(target).shape}\")\n",
    "            print(f\"\\nПервые строки:\")\n",
    "            print(df.head(2))\n",
    "\n",
    "print(\"\\nДанные готовы к обработке!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ceb73d",
   "metadata": {},
   "source": [
    "## Шаг 2.1: Анализ структуры данных\n",
    "\n",
    "Изучаем структуру датасета для понимания формата и выбора стратегии обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Анализ загруженных данных:\\n\")\n",
    "\n",
    "for file in os.listdir(\"data/raw\"):\n",
    "    filepath = os.path.join(\"data/raw\", file)\n",
    "    \n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"CSV файл: {file}\")\n",
    "        print(f\"  Размер: {df.shape}\")\n",
    "        print(f\"  Колонки: {list(df.columns)}\")\n",
    "        print(f\"  Пример данных:\\n{df.head(2)}\\n\")\n",
    "        \n",
    "    elif file.endswith('.json'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    print(f\"JSON файл: {file}\")\n",
    "                    print(f\"  Количество записей: {len(data)}\")\n",
    "                    print(f\"  Пример записи: {data[0]}\\n\")\n",
    "            except:\n",
    "                print(f\"JSON файл (построчный): {file}\")\n",
    "\n",
    "print(\"\\nСтратегия обработки:\")\n",
    "print(\"1. Извлечение текста вопросов пациентов и ответов врачей\")\n",
    "print(\"2. Создание меток: нужен ли офлайн-визит (на основе ключевых слов)\")\n",
    "print(\"3. Формирование датасета: [text, label]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git init\n",
    "\n",
    "!dvc init\n",
    "\n",
    "!git add .gitignore .dvc/config .dvc/.gitignore\n",
    "!git commit -m \"Initialize Git and DVC\"\n",
    "\n",
    "print(\"\\nGit и DVC инициализированы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78679513",
   "metadata": {},
   "source": [
    "## Шаг 4: Версионирование данных через DVC\n",
    "\n",
    "Добавляем датасет под контроль DVC и коммитим метаданные в Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc add data/raw/chatdoctor.csv\n",
    "\n",
    "!git add data/raw/chatdoctor.csv.dvc data/raw/.gitignore\n",
    "!git commit -m \"Add raw dataset via DVC\"\n",
    "\n",
    "print(\"\\nДатасет добавлен в DVC!\")\n",
    "print(\"Создан файл data/raw/chatdoctor.csv.dvc с метаданными\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc3750",
   "metadata": {},
   "source": [
    "## Шаг 5: Создание DVC пайплайна\n",
    "\n",
    "Создаем dvc.yaml с двумя стадиями: prepare и train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad14586",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_yaml_content = \"\"\"stages:\n",
    "  prepare:\n",
    "    cmd: python src/prepare.py\n",
    "    deps:\n",
    "      - src/prepare.py\n",
    "      - data/raw/chatdoctor.csv\n",
    "    params:\n",
    "      - prepare.test_size\n",
    "      - prepare.random_state\n",
    "      - prepare.sample_size\n",
    "    outs:\n",
    "      - data/processed/train.csv\n",
    "      - data/processed/test.csv\n",
    "\n",
    "  train:\n",
    "    cmd: python src/train.py\n",
    "    deps:\n",
    "      - src/train.py\n",
    "      - data/processed/train.csv\n",
    "      - data/processed/test.csv\n",
    "    params:\n",
    "      - train.model_type\n",
    "      - train.n_estimators\n",
    "      - train.max_depth\n",
    "      - train.random_state\n",
    "      - train.max_features\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dvc.yaml\", \"w\") as f:\n",
    "    f.write(dvc_yaml_content)\n",
    "\n",
    "print(\"Файл dvc.yaml создан!\")\n",
    "print(\"\\nСодержимое:\")\n",
    "print(dvc_yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2497ae3",
   "metadata": {},
   "source": [
    "## Шаг 6: Запуск DVC пайплайна\n",
    "\n",
    "Запускаем полный пайплайн: prepare → train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a54d8",
   "metadata": {},
   "source": [
    "## Шаг 7: Версионирование обработанных данных и модели\n",
    "\n",
    "Добавляем в DVC обработанные данные и модель, фиксируем изменения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add dvc.yaml dvc.lock params.yaml requirements.txt README.md .gitignore\n",
    "!git add src/prepare.py src/train.py\n",
    "!git add data/processed/.gitignore models/.gitignore\n",
    "!git add data/processed/train.csv.dvc data/processed/test.csv.dvc models/model.pkl.dvc\n",
    "\n",
    "!git commit -m \"Add DVC pipeline with prepare and train stages\"\n",
    "\n",
    "print(\"\\nВсе изменения закоммичены в Git!\")\n",
    "print(\"Метаданные данных и модели под контролем DVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65640e",
   "metadata": {},
   "source": [
    "## Шаг 8: Настройка удаленного хранилища DVC\n",
    "\n",
    "Создаем локальное хранилище для данных и моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdcc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../dvc-storage\", exist_ok=True)\n",
    "\n",
    "!dvc remote add -d localstorage ../dvc-storage\n",
    "!dvc push\n",
    "\n",
    "!git add .dvc/config\n",
    "!git commit -m \"Configure DVC remote storage\"\n",
    "\n",
    "print(\"\\nDVC remote storage настроен!\")\n",
    "print(\"Данные и модели загружены в ../dvc-storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed69e9",
   "metadata": {},
   "source": [
    "## Шаг 9: Просмотр результатов в MLflow\n",
    "\n",
    "Запускаем MLflow UI для просмотра экспериментов, метрик и моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "\n",
    "print(\"Для просмотра MLflow UI выполните в терминале:\")\n",
    "print(\"\\n  mlflow ui --backend-store-uri sqlite:///mlflow.db\")\n",
    "print(\"\\nЗатем откройте в браузере: http://127.0.0.1:5000\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "experiments = mlflow.search_experiments()\n",
    "print(\"\\nДоступные эксперименты:\")\n",
    "for exp in experiments:\n",
    "    print(f\"  - {exp.name} (ID: {exp.experiment_id})\")\n",
    "\n",
    "runs = mlflow.search_runs(experiment_names=[\"chatdoctor_classification\"])\n",
    "if not runs.empty:\n",
    "    print(f\"\\nЗапущено экспериментов: {len(runs)}\")\n",
    "    print(\"\\nПоследний эксперимент:\")\n",
    "    latest = runs.iloc[0]\n",
    "    print(f\"  Accuracy:  {latest['metrics.accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {latest['metrics.precision']:.4f}\")\n",
    "    print(f\"  Recall:    {latest['metrics.recall']:.4f}\")\n",
    "    print(f\"  F1-score:  {latest['metrics.f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e4630",
   "metadata": {},
   "source": [
    "## Шаг 10: Проверка воспроизводимости\n",
    "\n",
    "Проверяем структуру проекта и готовность к воспроизведению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Проверка структуры проекта:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_files = [\n",
    "    \"dvc.yaml\",\n",
    "    \"params.yaml\",\n",
    "    \"requirements.txt\",\n",
    "    \"README.md\",\n",
    "    \".gitignore\",\n",
    "    \"src/prepare.py\",\n",
    "    \"src/train.py\"\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    exists = \"✓\" if os.path.exists(file) else \"✗\"\n",
    "    print(f\"{exists} {file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nПроект готов к публикации!\")\n",
    "print(\"\\nДля воспроизведения на другой машине:\")\n",
    "print(\"  1. git clone <repo>\")\n",
    "print(\"  2. cd <repo>\")\n",
    "print(\"  3. pip install -r requirements.txt\")\n",
    "print(\"  4. dvc pull\")\n",
    "print(\"  5. dvc repro\")\n",
    "print(\"\\nMLflow UI:\")\n",
    "print(\"  mlflow ui --backend-store-uri sqlite:///mlflow.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea81f0",
   "metadata": {},
   "source": [
    "## Итоги и оптимизация для MacBook Pro M4\n",
    "\n",
    "### Реализованная задача\n",
    "**Бинарная классификация медицинских консультаций:**\n",
    "- Класс 0: Консультация решается онлайн\n",
    "- Класс 1: Требуется офлайн-визит к врачу\n",
    "\n",
    "### ML Pipeline\n",
    "1. **prepare.py**: Парсинг данных ChatDoctor, автогенерация меток, train/test split\n",
    "2. **train.py**: TF-IDF + RandomForest, логирование метрик в MLflow\n",
    "3. **dvc.yaml**: Автоматизация через `dvc repro`\n",
    "\n",
    "### Оптимизация для M4\n",
    "- `n_jobs=-1`: использование всех ядер CPU\n",
    "- TF-IDF max_features=5000: баланс качества и скорости\n",
    "- Sample size 10000: быстрое обучение для экспериментов\n",
    "- `max_depth=15`: предотвращение переобучения\n",
    "\n",
    "### Альтернативные модели для M4\n",
    "- **LogisticRegression**: быстрая, хорошо работает с текстом\n",
    "- **SGDClassifier**: для очень больших датасетов\n",
    "- **ComplementNB**: эффективен для несбалансированных классов\n",
    "\n",
    "### Следующие шаги\n",
    "1. Загрузить в GitHub\n",
    "2. Настроить DVC remote (локальный или S3)\n",
    "3. Экспериментировать с параметрами через `params.yaml`\n",
    "4. Добавить evaluate стадию (опционально)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
